# Guides: Configure Calico for Networking (WIP)

I'm not sure if this was ever finished, but left in case someone wants to
continue with it. Or clean it up.

In any case I deemed this install  too complicated for what I needed to know at
the time and moved on to Antrea, and then `kube-routed`.
There installations were more intuitive and less messy. Though
in all 3 cases, documentation was a hot mess, no clear path, and you had to
figure out little nuances as you installed.

We can use [Calico the hard way].
1. Install `calicoctl`
   ```shell
   export CALICO_VER=v3.30.2
   wget -O calicoctl https://github.com/projectcalico/calico/releases/download/${CALICO_VER}/calicoctl-linux-amd64
   chmod +x calicoctl
   sudo mv calicoctl /usr/local/bin/
   # set every time we login.
   sudo cp ~/.profile ~/profile.bak
   cat <<TXT | sudo tee -a ~/.profile
   export KUBECONFIG=~/.kube/config
   export DATASTORE_TYPE=kubernetes
   TXT
   export KUBECONFIG=~/.kube/config
   export DATASTORE_TYPE=kubernetes
   ```
2. Set up a certificate for the `calico-ipam` CNI plugin to authenticate with
   `kube-apiserver`:
   ```shell
   cd ~/certs
   CERT_BITS=2048
   CERT_DAYS=365
   cert_dir=/etc/kubernetes/pki
   CLUSTER_NAME=kubernetes
   CLUSTER_ENDPOINT=https://192.168.56.11:6443
   CNI_NAME="calico-ipam"

   openssl req -newkey rsa:${CERT_BITS} \
        -keyout ${CNI_NAME}.key \
        -nodes \
        -out ${CNI_NAME}.csr \
        -subj "/CN=calico-cni"

   sudo openssl x509 -req -in cni.csr \
               -CA ${cert_dir}/ca.crt \
               -CAkey ${cert_dir}/ca.key \
               -CAcreateserial \
               -out ${CNI_NAME}.crt \
               -days ${CERT_DAYS}
   ```
3. Make a kubeconfig for the CNI plugin to use to access `kube-apiserver`.
   ```shell
   kubectl config set-cluster ${CLUSTER_NAME} \
   --certificate-authority=${cert_dir}/ca.crt \
   --embed-certs=true \
   --server=${CLUSTER_ENDPOINT} \
   --kubeconfig=${CNI_NAME}.conf

   kubectl config set-credentials calico-cni \
   --client-certificate=${CNI_NAME}.crt \
   --client-key=${CNI_NAME}.key \
   --embed-certs=true \
   --kubeconfig=${CNI_NAME}.conf

   kubectl config set-context default \
   --cluster=${CLUSTER_NAME} \
   --user=calico-cni \
   --kubeconfig=${CNI_NAME}.conf

   kubectl config use-context default --kubeconfig=${CNI_NAME}.conf
   sudo cp ${CNI_NAME}.conf /etc/kubernetes
   ```
4. Define a cluster role for the Calico CNI plugin:
   ```shell
   kubectl apply -f /vagrant/calico-cni-cluster-role.yaml
   ```
5. Bind the ClusterRole to the calico account:
   ```shell
   kubectl create clusterrolebinding calico-cni --clusterrole=calico-cni --user=calico-cni
   ```
6. NOTE: These are hard to get if you don't have Tigera install them. The
   website for the hard way list a very old version.
   ```shell
   wget https://github.com/projectcalico/cni-plugin/releases/download/${CALICO_VER}/calico-amd64
   sudo install -m 755 calico-amd64 /opt/cni/bin/calico
   wget https://github.com/projectcalico/cni-plugin/releases/download/${CALICO_VER}/calico-ipam-amd64
   sudo install -m 755 calico-ipam-amd64 /opt/cni/bin/calico-ipam
   ```
7. Write the Calico CNI configuration
   ```shell
   cat <<EOF | sudo tee /etc/cni/net.d/10-calico.conflist
   {
     "name": "k8s-pod-network",
     "cniVersion": "0.3.1",
     "plugins": [
       {
         "type": "calico",
         "log_level": "info",
         "datastore_type": "kubernetes",
         "mtu": 1500,
         "ipam": {
             "type": "calico-ipam",
             "assign_ipv4": "true",
             "assign_ipv6": "true",
         },
         "policy": {
             "type": "k8s"
         },
         "kubernetes": {
             "kubeconfig": "/etc/kubernetes/calico-ipam.conf"
         }
       },
       {
         "type": "portmap",
         "snat": true,
         "capabilities": {"portMappings": true}
       }
     ]
   }
   EOF
   ```
8. Install Typha:
   ```shell
   cd ~/certs
   CERT_BITS=2048
   CERT_DAYS=365
   typha_cert_dir=/etc/kubernetes/pki/typha
   sudo mkdir -p ${typha_cert_dir}
   CLUSTER_NAME=kubernetes
   CLUSTER_ENDPOINT=https://192.168.56.11:6443

   sudo openssl req -x509 -newkey rsa:${CERT_BITS} \
               -keyout ${typha_cert_dir}/ca.key \
               -nodes \
               -out ${typha_cert_dir}/ca.crt \
               -subj "/CN=Calico Typha CA" \
               -days ${CERT_DAYS}

   # Store the CA certificate in a ConfigMap that Typha & calico/node will access.
   sudo kubectl create configmap -n kube-system calico-typha-ca --from-file=${typha_cert_dir}/ca.crt \
     --kubeconfig /etc/kubernetes/super-admin.conf

   # Make the Typha client key and certificate signing request (CSR)
   openssl req -newkey rsa:${CERT_BITS} \
        -keyout typha.key \
        -nodes \
        -out typha.csr \
        -subj "/CN=calico-typha"
   # Sign the Typha client certificate with the Typha CA.
   sudo openssl x509 -req -in typha.csr \
               -CA ${typha_cert_dir}/ca.crt \
               -CAkey ${typha_cert_dir}/ca.key \
               -CAcreateserial \
               -out typha.crt \
               -days ${CERT_DAYS}
   # Store the Typha client key and certificate in a secret that Typha will access.
   kubectl create secret tls -n kube-system calico-typha-certs \
      --cert=typha.crt \
      --key=typha.key
   ```
9. Provision RBAC:
   ```shell
   # Make a ServiceAccount that will be used to run Typha.
   kubectl create serviceaccount -n kube-system calico-typha
   ```
10. Define a cluster role for Typha with permission to watch Calico datastore objects.
    ```shell
    kubectl apply -f /vagrant/calico-typha-cluster-role.yaml
    ```
11. Bind the cluster role to the calico-typha ServiceAccount.
    ```shell
    kubectl create clusterrolebinding calico-typha --clusterrole=calico-typha --serviceaccount=kube-system:calico-typha
    ```
12. Install Typha Deployment:

    ```shell
    kubectl apply -f /vagrant/calico-typha-deploy.yml
    ```
13. Install Service calico/node uses to get load-balanced access to Typha.
    ```shell
    kubectl apply -f /vagrant/calico-typha-service.yaml
    ```

    Apply and test
    ```shell
    TYPHA_CLUSTERIP=$(kubectl get svc -n kube-system calico-typha -o jsonpath='{.spec.clusterIP}')
    sudo curl https://$TYPHA_CLUSTERIP:5473 -v --cacert /etc/kubernetes/pki/typha/ca.crt
    ```
    NOTE: Don't worry about the "bad certificate" you see in the output,
    that is expected at this point according to the Calico documentation.
14. Make the key calico/node will use to authenticate with Typha and the
    certificate signing request (CSR).
    ```shell
    openssl req -newkey rsa:${CERT_BITS} \
        -keyout calico-node.key \
        -nodes \
        -out calico-node.csr \
        -subj "/CN=calico-node"

    # Sign the Felix certificate with the CA we created earlier
    sudo openssl x509 -req -in calico-node.csr \
               -CA ${typha_cert_dir}/ca.crt \
               -CAkey ${typha_cert_dir}/ca.key \
               -CAcreateserial \
               -out calico-node.crt \
               -days ${CERT_DAYS}

    # Store the key and certificate in a Secret that calico/node will access
    kubectl create secret tls -n kube-system calico-node-certs --key=calico-node.key --cert=calico-node.crt

    # Validate the key and certificate that calico/node will use to access Typha using TLS:
    sudo curl https://calico-typha:5473 -v --cacert /etc/kubernetes/pki/typha/ca.crt \
         --resolve calico-typha:5473:${TYPHA_CLUSTERIP} --cert calico-node.crt --key calico-node.key
    ```
15. Make the ServiceAccount that calico/node will run as.
    ```shell
    kubectl create serviceaccount -n kube-system calico-node
    ```
16. Provision a ClusterRole with permissions to read and modify Calico
    datastore objects.
    ```shell
    kubectl apply -f /vagrant/calico-node-cluster-role-binding.yaml
    ```
17. Bind the cluster role to the calico-node ServiceAccount.
    ```shell
    kubectl create clusterrolebinding calico-node --clusterrole=calico-node --serviceaccount=kube-system:calico-node
    ```
18. calico/node runs as a daemon set so that it is installed on every node
    in the cluster. Make the daemon set.
    ```shell
    kubectl apply -f /vagrant/calico-node-daemonset.yaml
    ```
---

[Calico the hard way]: https://docs.tigera.io/calico/latest/getting-started/kubernetes/hardway/overview
