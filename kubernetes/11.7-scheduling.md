# Scheduler

Every Pod has a field `nodeName` that by default is not set.
1. The scheduler goes though all the Pods and looks for those that do not have
   this property set, and selects them as candidates for scheduling.
2. It then selects a Pod and runs an algorithm to identify the right node for
   the Pod.
3. It then schedules the Pod to be placed on the node by setting the `nodeName`
   to the name of the node, for the Pod, by creating a binding object.

## Manual Scheduling

To manually assign a node you have 2 ways:
1. You can assign a nodeName at the time of making the new Pod.
2. Make a binding object, as the scheduler does, for the Pod, but this must be sent as a JSON object over to the Pods biding API with the data:
   ```yaml
   apiVersion: v1
   kind: Binding
   metadata:
     name: nginx
   target:
     apiVersion: v1
     kind: Node
     name: node02
   ```
   ```sh
   SERVER='control-plane:6443'
   POD_NAME="nginx"
   cat <<HDOC > pod-binding-data.json
   {
     "apiVersion": "v1",
     "kind": "Binding",
     "metadata": {
        "name": "nginx",
     }
     "target": {
       "apiVersion": "v1",
       "kind": "Node",
       "name": "worker-01"
     }
   }
   HDOC

   curl -k --header "Content-Type: application/json" \
       --header "Authorization: Bearer ${K8_TOKEN}" \
       --request POST --data @./pod-binding-data.json \
       https://${SERVER}/api/v1/namespaces/default/pods/${POD_NAME}/binding
   ```

## Configure Kubernetes Scheduler

```yaml
apiVersion: v1
kind: Pod
metadata:
   name: multi-pod
   labels:
      name: multi-pod
spec:
   containers:
      - name: alpha
        image: nginx
        env:
           - name: "name"
             value: "alpha"
      - name: beta
        image: busybox
        command: [ sleep, "4800" ]
        env:
           - name: "name"
             value: "beta"
---
apiVersion: v1
kind: Pod
metadata:
   name: non-root-pod
   labels:
      name: non-root-pod
spec:
   securityContext:
      runAsUser: 1000
      fsGroup: 2000
   containers:
      - name: non-root-pod
        image: redis:alpine
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
   name: ingress-to-nptest
spec:
   podSelector:
      matchLabels:
         run: np-test-1
   policyTypes:
      - Ingress
   ingress:
      - ports:
           - protocol: TCP
             port: 80
---
apiVersion: v1
kind: Pod
metadata:
   name: prod-redis
   labels:
      name: prod-redis
spec:
   containers:
      - name: prod-redis
        image: redis:alpine
   tolerations:
      - key: "env_type"
        operator: "Equal"
        value: "production"
        effect: "NoSchedule"
---
apiVersion: v1
kind: Pod
metadata:
   name: hr-pod
   namespace: hr
   labels:
      name: hr-pod
      environment: production
      tier: frontend
spec:
   containers:
      - name: hr-pod
        image: redis:alpine
```

## Taints

Allow a node to repel a set of Pods.

You add a taint to a node using kubectl taint. For example,
`kubectl taint nodes node1 key1=value1:NoSchedule`

To remove the taint added by the command above, you can run:
`kubectl taint nodes node1 key1=value1:NoSchedule-`

The node controller automatically taints a Node when certain conditions are
true. The following taints are built in:
* node.kubernetes.io/not-ready: Node is not ready. This corresponds to the
  NodeCondition Ready being "False".
* node.kubernetes.io/unreachable: Node is unreachable from the node controller.
  This corresponds to the NodeCondition Ready being "Unknown".
* node.kubernetes.io/memory-pressure: Node has memory pressure.
* node.kubernetes.io/disk-pressure: Node has disk pressure.
* node.kubernetes.io/pid-pressure: Node has PID pressure.
* node.kubernetes.io/network-unavailable: Node's network is unavailable.
* node.kubernetes.io/unschedulable: Node is unschedulable.
* node.cloudprovider.kubernetes.io/uninitialized

## Tolerations

Tolerations are applied to Pods. Tolerations allow the scheduler to schedule
Pods with matching taints. While they allow scheduling, they don't guarantee it,
since scheduling has to make other considerations such as CPU resources.

A `toleration` "matches" a `taint` if the keys are the same and the effects are
the same, and:
* the operator is `Exists` (in which case no value should be specified), or
* the operator is Equal and the values should be equal.

You specify a toleration for a pod in the PodSpec.
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx
  labels:
    env: test
spec:
  containers:
  - name: nginx
    image: nginx
    imagePullPolicy: IfNotPresent
  tolerations:
  - key: "example-key"
    operator: "Exists"
    effect: "NoSchedule"
```

The default Kubernetes scheduler takes taints and tolerations into account
when selecting a node to run a particular Pod. However, if you manually
specify the .spec.nodeName for a Pod, that action bypasses the scheduler;
the Pod is then bound onto the node where you assigned it, even if there are
NoSchedule taints on that node that you selected. If this happens and the node
also has a NoExecute taint set, the kubelet will eject the Pod unless there is
an appropriate tolerance set.

There are two special cases:
* If the `key` is empty, then the operator must be `Exists`, which matches all
  keys and values. Note that the effect still needs to be matched at the same
  time.
* An empty effect matches all effects with key `key1`.

The allowed values for the effect field are:
* NoExecute
* NoSchedule
* PreferNoSchedule