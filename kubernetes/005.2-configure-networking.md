# Configure Networking

Before we begin, lets see what bridge networks we have so far:
`sudo bridge link show`. Just make a mental note, or paste the output for
reference later.

1. Add a default kubeconfig to access the cluster.
   ```shell
   mkdir -p $HOME/.kube
   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
   sudo chown $(id -u):$(id -g) $HOME/.kube/config
   ```
2. Install a CNI addon, you have several choices:
    1. Install Flannel CNI:
        1. Download the manifest.
           ```shell
           wget https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml
           ```
        2. Edit container spec to set the network interface to use:
           ```yaml
           containers:
            - args:
              - --ip-masq
              #...
              - --iface=enp0s8
           ```
           NOTE: Careful! Even if you use this [Vagrantfile], the network
           card may have a different name, be sure to check what name it has
           with `ip address` on the control plane VM.
        3. Optionally edit `vi kube-flannel.yml` to add IPv6:
           ```yaml
           data:
             net-conf.json: |
               {
                 "Network": "10.244.0.0/16",
                 "EnableNFTables": false,
                 "Backend": {
                  "Type": "vxlan"
                 },
                 "EnableIPv6": true,
                 "IPv6Network": "2001:db8:42:0::/56"
               }
           ```
    2. To get practice with Network Policies, install the Antrea addon.
        1. Install OvS on all nodes.
           ```shell
           sudo apt install -y openvswitch-switch openvswitch-common
           ```

           NOTE: See https://docs.openvswitch.org/en/latest/intro/install/debian/#installing-deb-packages

        2. Install Antrea via manifest:
           This uses [NodeIPAM within kube-controller-manager]
           ```shell
           ANTREA_VER=v2.5.1
           wget https://github.com/antrea-io/antrea/releases/download/${ANTREA_VER}/antrea.yml
           ```
        3. Modify the Antrea configuration to use the CIDRs previously chosen:
           ```text
           serviceCIDR: "10.96.0.0/16"
           serviceCIDRv6: "2001:db8:42:1::/112"
           transportInterface: "enp0s8"`
           ```
        4. Then apply it `kubectl apply -f antrea.yml`
           NOTE: The antrea.yml is over 6000 lines long. And has a Deployment
           the `antrea-controller` which take up 200m CPU and another DaemonSet
           the `antrea-agent` which will take up another 200m CPU on control-planes.
           That's 400m altogether, along with about 650m for the control-plane
           components. That's a total of 1050m CPU, so you'll need at least 2 CPUs
           per control-plane.

           NOTE: ClusterIP CIDR range for Services is only required when
           `AntreaProxy` is not enabled.
    3. Another alternative, install `kube-router` addon, which also provides
       Network Policy features, and we'll also replace `kube-proxy`.
       `kube-router` needs to access kubernetes the API server to get information
       on pods, services, endpoints, network policies etc.
        1. When run as `DaemonSet`, the container image is prepackaged with
           `ipset`, but it MUST also be installed on each node user:
           ```shell
           sudo apt-get install -y ipset ipvsadm
           ```
        2. We'll generate a certificate for `kube-router` so it can access the
           `kube-apiserver`:
           ```shell
           getIps () {
               iface=${1}
               export IPV4=$(ip -4 addr show scope global dev ${iface} | grep "inet\b" | awk '{print $2}' | cut -d/ -f1)
               export IPV6=$(ip -6 addr show scope global dev ${iface} | grep "inet6\b" | awk '{print $2}' | cut -d/ -f1)
           }
  
           sudo mkdir -p ${cert_dir}
           cd ~/certs
           getIps "enp0s8"
           envsubst < /vagrant/openssl-kube-router.conf.tmpl > openssl-kube-router.conf
  
           cert_dir=/etc/kubernetes/pki
           CERT_DAYS=365
           CERT_BITS=2048
           CERT_CONF="openssl-kube-router.conf"
           CLUSTER_NAME=kubernetes
           CLUSTER_ENDPOINT=https://${IPV4}:6443
           i="kube-router"
  
           openssl genrsa -out "${i}.key" ${CERT_BITS}
  
           openssl req -new -key "${i}.key" -sha256 \
            -config "${CERT_CONF}" -section ${i} \
            -out "${i}.csr"
  
           # sign the CSR with the etcd-ca key.
           sudo openssl x509 -req -days ${CERT_DAYS} -in "${i}.csr" \
            -copy_extensions copyall \
            -sha256 -CA "${cert_dir}/ca.crt" \
            -CAkey "${cert_dir}/ca.key" \
            -CAcreateserial \
            -out "${i}.crt"
           ```
        3. Make a kubeconfig to mount inside the `kube-router`'s Static Pod, so
           it will have access to the `kube-apiserver`:
           ```shell
           mkdir -p ~/certs && cd ~/certs
           i="kube-router"
           cert_dir=/etc/kubernetes/pki
           CLUSTER_NAME=kubernetes
           CLUSTER_ENDPOINT=https://${IPV4}:6443
  
           # Make a kubeconfig for the node adding the cluster.
            sudo kubectl config set-cluster ${CLUSTER_NAME} \
             --certificate-authority=${cert_dir}/ca.crt \
             --embed-certs=true \
             --server=${CLUSTER_ENDPOINT} \
             --kubeconfig=/etc/kubernetes/${i}.conf
            # Add credentials to the kubeconfig.
            sudo kubectl config set-credentials ${i} \
             --client-certificate=${i}.crt \
             --client-key=${i}.key \
             --embed-certs=true \
             --kubeconfig=/etc/kubernetes/${i}.conf
           # Add a context to the cluster.
           sudo kubectl config set-context default \
             --cluster=${CLUSTER_NAME} \
             --user=${i} \
             --kubeconfig=/etc/kubernetes/${i}.conf
           # Set the context to use by default in the kubeconfig.
           sudo kubectl config use-context default \
             --kubeconfig=/etc/kubernetes/${i}.conf
           sudo mkdir -p /var/lib/kube-router
           sudo cp -p /etc/kubernetes/kube-router.conf /var/lib/kube-router/kubeconfig
           ```
        4. Make a `kube-router` manifest:
           ```shell
           mkdir ~/manifests && cd ~/manifests
           wget -O kube-router-all.yml https://raw.githubusercontent.com/cloudnativelabs/kube-router/master/daemonset/generic-kuberouter-all-features.yaml
           ```
        5. Apply changes to the `kube-router` manifest
           ```text
           wget -O kube-router-all.yaml.tmpl https://raw.githubusercontent.com/cloudnativelabs/kube-router/master/daemonset/generic-kuberouter-all-features.yaml
           ```
        6. Edit the `kube-router-all-yaml` and add a user subject to the
           `ClusterRoleBinding` like so:
           ```yaml
           ---
           kind: ClusterRoleBinding
           #...
           subjects:
           - #...
           - kind: User # Add the user to the subject for using with the kubeconfig we generated.
             name: kube-router
             apiGroup: rbac.authorization.k8s.io
           ```
        7. Fill in placeholders then apply the manifest:
           ```shell
           CLUSTERCIDR=10.96.0.0/16,2001:db8:42:1::/112 \
           APISERVER=https://control-plane:6443 \
           sh -c 'cat kube-router-all.yaml.tmpl | \
           sed -e "s;%APISERVER%;$APISERVER;g" -e "s;%CLUSTERCIDR%;$CLUSTERCIDR;g"' | tee kube-router-all.yaml
           kubectl apply -f kube-router-all.yaml
           #  --enable-cni
           #  --enable-ipv4
           #  --enable-ipv6
           #  --service-cluster-ip-range="10.96.0.0/16"
           #  --service-cluster-ip-range="2001:db8:42:1::/112"
           ```
3. Test that DNS resolution for the cluster is working:
    1. Launch a Pod with DNS tools:
       `kubectl run -it --rm --restart=Never --image=infoblox/dnstools:latest dnstools`
    2. Query the DNS server:
       `host kubernetes`
    3. Now type `exit` and it should remove the  `dnstools` Pod since we used
       `--rm` flag.

Next: [Add Workers]

---

[CoreDNS Helm chart]: https://github.com/coredns/helm
[Add Workers]: /kubernetes/005.3-add-workers.md
[NodeIPAM within kube-controller-manager]: https://antrea.io/docs/main/docs/getting-started/#nodeipam-within-kube-controller-manager