# Guides: Cluster Maintenance

If a node goes down for 5 minutes, then the pods are terminated and Kubernetes
considered them dead. If they are part of a replicaset, they will be replicated
to other nodes. If not then they are just gone.

When or if the node comes back up after 5 minutes, it should come up blank
without any PODs scheduled on it.

You can perform a quick upgrade on a single node if you know that:

1. All PODs on the node are deployments or replication sets, since they will automatically move to another node when you drain it. This is NOT true for manually deployed POds.
2. And it is OK that they go down for a short period of time
3. And that the node will come back online within 5 minutes.
4. Then it should be OK to perform the upgrade. But don't really do this unless it's an emergency.

Instead, drain the node. This should recreate pods on other nodes. Also marking the node as cordoned will prevent scheduling new PODs on it. When you're done, then you can run `uncordon <node-name>`; which will me it schedule-able again.

So the safer process to upgrade a node is:
`k drain node-1`

This redeploys PODs to other nodes, then marks the node as `cordon`'ed, which prevent any new PODs from being scheduled on the node.

When you are done with the upgrade, you can mark node as `uncordon` which will put it back in servers and allow scheduling PODs on the node again.
`k uncordon node-1`

## Cluster Upgrade

All Control Plane components do NOT have to be at the same version. The `controller-manager` and `kube-scheduler` can be 1 version lower; while `kubelet` and `kube-proxy` can be 1 version lower or higher. The `kubectl` command should be on the same version or 1 lower.

ETCD and will have different version because they are separate projects, but refer to the cluster installation guide for recommended versions.

When should you upgrade? Kube only support the last 3 versions. So you should only keep 2 or 3 releases behind. Plus you should be experimenting in a non-prod environment with the version you plan to upgrade to months beforehand.

If you deployed with `kubeadm`, then you can upgrade with:

```shell
kubeadm upgrade plan
kubeadm upgrade apply
```

1. Upgrade your control-plane nodes.
   * While management is down you cannot deploy PODs. However, the clusters exising PODs should continue to function.
2. Upgrade your worker nodes.

### Strategies

1. Upgrade all nodes at the same time.
2. Upgrade 1 at a time.
3. Add new nodes with an upgraded version to replace a node, repeat until all nodes are upgraded. Good for Cloud clusters.

IMPORTANT: You MUST upgrade the `kubeadm` tool before upgrading the cluster, and you can only upgrade 1 minor version at a time. So baby steps.

NOTE: `kubeadm` does not install the `kubelet` so you will have to manually upgrade these on your own. It will notify you of this (strange, it could just do it for us).

To upgrade the `kubelet` on a node you can use the system package manager, remember only 1 minor version at a time:

```shell
@controlplan> kubectl drain node01
@node01> apt-get upgrade -y kubeadm=1.12.0-00
@node01> kubeadm upgrade node config --kubelet-version v1.12.0
@node01> apt-get upgrade -y kubelet=1.12.0-00
@node01> systemctl restart kubelet
@controlplan> k uncordon node01
```

## Upgrading kubeadm clusters

For details visit [Upgrading kubeadm clusters]
https://v1-31.docs.kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/

1. Pick a controlplane node to upgrade first.
   1. Drain the node:
      ```shell
      kubectl drain controlplane --ignore-daemonsets
      ```
      NOTE: Remember this MUST be done from a CLI with kubectl has access to the cluster, so make sure the context is set correctly.
   2. With an etcd upgrade, in-flight requests to the server will stall while the new etcd static pod is restarting. Stop the kube-apiserver process a few seconds before starting the kubeadm upgrade apply command to complete in-flight requests and close existing connections, and minimizes the consequence of the etcd downtime.
      ```shell
      killall -s SIGTERM kube-apiserver # trigger a graceful kube-apiserver shutdown
      sleep 20 # wait a little bit to permit completing in-flight requests
      ```
   3. Changing the package repository:
      Update the following file with the next minor version you want to use. Also make sure its using `https://pkgs.k8s.io/core:/stable`
      ```shell
      cat /etc/apt/sources.list.d/kubernetes.list
      sudo vi /etc/apt/sources.list.d/kubernetes.list
      sudo apt update
      sudo apt-cache madison kubeadm
      ```
   4. Upgrade kubeadm:
      ```shell
      sudo apt-mark unhold kubeadm kubelet && \
      sudo apt-get update && sudo apt-get install -y kubeadm="1.31.0-*" kubelet="1.31.0-*" && \
      sudo apt-mark hold kubeadm
      ```
   5. Verify that the download works and has the expected version:
      ```shell
      kubeadm version
      ```
   6. Verify the upgrade plan:
      ```shell
      sudo kubeadm upgrade plan
      ```
      NOTE: This is ONLY run on the first node you upgrade.
   7. Upgrade:
      ```shell
      sudo kubeadm upgrade apply "v1.31.0"
      ```
      NOTE: This is ONLY run on the first node you upgrade. Once the command finishes you should see:
      ```shell
      [upgrade/successful] SUCCESS! Your cluster was upgraded to "v1.31.x". Enjoy!

      [upgrade/kubelet] Now that your control plane is upgraded, please proceed with upgrading your kubelets if you haven't already done so.
      ```
   9. Restart the kubelet:
      ```shell
      sudo systemctl daemon-reload
      sudo systemctl restart kubelet
      ```
   8. Manually upgrade your CNI provider plugin.
      If you have a CNI plugin, then upgrade in on the first controlplane node, you do not need to do this on other controlplane nodes.
   9. For the other control plane nodes:
      Same as the first control plane node but use:
      ```shell
      sudo kubeadm upgrade node
      ```
   10. Upgrade kubelet and kubectl:
       ```shell
       sudo apt-mark unhold kubelet kubectl && \
       sudo apt-get update && sudo apt-get install -y kubelet="1.31.0-*" kubectl="1.31.0-*" && \
       sudo apt-mark hold kubelet kubectl
       ```
   11. Restart the kubelet:
      ```shell
      sudo systemctl daemon-reload
      sudo systemctl restart kubelet
      ```
   12. Uncordon the node:
      ```shell
      kubectl uncordon <node-to-uncordon>
      ```
2. Upgrade worker nodes
   1. Changing the package repository:
      Update the following file with the next minor version you want to use. Also make sure its using `https://pkgs.k8s.io/core:/stable`
      ```shell
      cat /etc/apt/sources.list.d/kubernetes.list
      sudo nano /etc/apt/sources.list.d/kubernetes.list
      ```
   2. Upgrade kubeadm:
      ```shell
      export K8_VER='1.31.0-1.1'
      sudo apt-mark unhold kubeadm && \
      sudo apt-get update && apt-get install -y kubeadm="${K8_VER}" && \
      sudo apt-mark hold kubeadm
      ```
   3. Call "kubeadm upgrade":
      ```shell
      sudo kubeadm upgrade node
      ```
   4. Drain the node:
      ```shell
      kubectl drain <node-to-drain> --ignore-daemonsets
      ```
   5. Upgrade kubelet and kubectl:
      ```shell
      export K8_VER='1.31.0-1.1'
      sudo apt-mark unhold kubelet kubectl && \
      sudo apt-get update && sudo apt-get install -y kubelet="${K8_VER}" kubectl="${K8_VER}" && \
      sudo apt-mark hold kubelet kubectl
      ```
   6. Restart the kubelet:
      ```shell
      sudo systemctl daemon-reload
      sudo systemctl restart kubelet
      ```
   7. Uncordon the node:
      ```shell
      kubectl uncordon <node-to-uncordon>
      ```
3. Verify the status of the cluster
   ```shell
   kubectl get nodes
   ```

## Backup and Restore Methods

Candidates for backup:
* Resource configurations
* Ectd
* Persistent volumes.

To back-up all the resource configurations in your cluster you can run:

```shell
k get all -A -o yaml > all-deployed-services.yaml
```

For `etcd` all of the data can be backed up that lives in the data-dir
`--data-dir=/var/lib/etcd` or whatever the configuration may be.

Etcd comes with a built-in snapshot solution.
```shell
ETCDCTL_API=3 etcdctl snapshot save snapshot.db
```

View the status of the snapshot using the status command:

```shell
ETCDCTL_API=3 etcdctl snapshot status snapshot.db
```

To restore from an etcd snapshot:
1. Stop the kube-apiserver service:
   ```shell
   sudo service kube-apiserver stop
   ```
2. Restore the etcd snapshot:
   ```shell
   ETCDCTL_API=3 etcdctl snapshot restore snapshot.db \
      --data-dir /var/lib/etcd-from-backup
   ```
3. You then configure the etcd.service to use this new directory.
4. Reload the service daemon:
   ```shell
   systemctl daemon-reload
   sudo service etcd start
   ```
5. Start the kube-apiserver service
   ```shell
   service kube-apiserver start
   ```

NOTE: With all the `etcd` command remember to specify:
```shell
--endpoints=https://127.0.0.1:2379 \
--cacert=/etc/etcd/ca.cert \
--cert=/etc/etcd/etcd-server.crt \
--key=/etc/etcd/etcd-server.key

etcd
   --advertise-client-urls=https://192.168.231.146:2379
   --cert-file=/etc/kubernetes/pki/etcd/server.crt
   --client-cert-auth=true
   --data-dir=/var/lib/etcd
   --experimental-initial-corrupt-check=true
   --experimental-watch-progress-notify-interval=5s
   --initial-advertise-peer-urls=https://192.168.231.146:2380
   --initial-cluster=controlplane=https://192.168.231.146:2380
   --key-file=/etc/kubernetes/pki/etcd/server.key
   --listen-client-urls=https://127.0.0.1:2379,https://192.168.231.146:2379
   --listen-metrics-urls=http://127.0.0.1:2381
   --listen-peer-urls=https://192.168.231.146:2380
   --name=controlplane
   --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
   --peer-client-cert-auth=true
   --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
   --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
   --snapshot-count=10000
   --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
```

```shell
ETCDCTL_API=3 etcdctl snapshot save /opt/snapshot-pre-boot.db \
--endpoints=https://127.0.0.1:2379 \
   --cert=/etc/kubernetes/pki/etcd/server.crt \
   --key=/etc/kubernetes/pki/etcd/server.key \
   --cacert=/etc/kubernetes/pki/etcd/ca.crt

ETCDCTL_API=3 etcdctl snapshot status /opt/snapshot-pre-boot.db \
--endpoints=https://127.0.0.1:2379 \
   --cert=/etc/kubernetes/pki/etcd/server.crt \
   --key=/etc/kubernetes/pki/etcd/server.key \
   --cacert=/etc/kubernetes/pki/etcd/ca.crt

ETCDCTL_API=3 etcdctl snapshot restore /opt/snapshot-pre-boot.db \
   --data-dir=/var/lib/etcd-from-backup

ETCDCTL_API=3 etcdctl snapshot restore /opt/cluster2.db \
   --endpoints=https://192.2.110.10:2379 \
   --data-dir=//var/lib/etcd-data-new
```

---

[Upgrading kubeadm clusters]: https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/
