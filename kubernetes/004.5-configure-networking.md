# Configure Networking

Before we begin, lets see what bridge networks we have so far:
`sudo bridge link show`. Just make a mental note, or paste the output for
reference later.

1. Add a default kubeconfig to access the cluster.
   ```shell
   mkdir -p ~/.kube
   sudo cp /etc/kubernetes/admin.conf ~/.kube/config
   sudo chown vagrant:vagrant ~/.kube/config
   chmod 0700 ~/.kube/config
   sudo kubectl create clusterrolebinding \
     kubernetes-admin --clusterrole=cluster-admin --user=kubernetes-admin \
     --kubeconfig /etc/kubernetes/super-admin.conf
   ```
2. Copy the `kube-proxy.yaml.tmpl` file to `control-plane-01`.
   ```shell
   cp /vagrant/kube-proxy.yaml.tmpl .
   ```
3. Fill in the values for the `kube-proxy` config and then apply the manifest.
   ```shell
   export ADVERTISE_ADDRESS=192.168.56.11
   export POD_CIDRS=10.244.0.0/16,2001:db8:42:0::/56
   export SERVICE_CIDRS=10.96.0.0/16,2001:db8:42:1::/112
   export KUBE_VER=v1.33.3
   envsubst < kube-proxy.yaml.tmpl | tee kube-proxy.yaml
   kubectl apply -f kube-proxy.yaml
   ```
4. Install Helm
   ```shell
   wget https://get.helm.sh/helm-v3.18.4-linux-amd64.tar.gz
   sudo tar Czxvf /usr/local/bin/ helm-v3.18.4-linux-amd64.tar.gz --strip-components=1
   ```
5. Install CoreDNS using the [CoreDNS Helm chart].
   ```shell
   cp /vagrant/coredns-values.yaml .
   helm repo add coredns https://coredns.github.io/helm
   helm repo update
   helm --namespace=kube-system install coredns coredns/coredns -f coredns-values.yaml
   ```
6. Install and CNI addon, you have several choices:
   1. Install Flannel CNI:
      1. Download the manifest.
         ```shell
         wget https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml
         ```
      2. Edit container spec to set the network interface to use:
         ```yaml
         containers:
          - args:
            - --ip-masq
            #...
            - --iface=enp0s8
         ```
         NOTE: Be careful, even if you use this [Vagrantfile], the network card may
         have a different name, be sure to check what name it has with `ip address`
         on the control plane VM.
      3. Optionally edit `vi kube-flannel.yml` to add IPv6:
         ```yaml
         data:
           net-conf.json: |
             {
               "Network": "10.244.0.0/16",
               "EnableNFTables": false,
               "Backend": {
                "Type": "vxlan"
               },
               "EnableIPv6": true,
               "IPv6Network": "2001:db8:42:0::/56"
             }
         ```
   2. As an alternative to Flannel if you want to get practice with network
      policies, install the Antrea addon.
      1. Install OvS on all nodes.
         ```shell
         sudo apt install -y openvswitch-switch openvswitch-common
         ```

         NOTE: See https://docs.openvswitch.org/en/latest/intro/install/debian/#installing-deb-packages

      2. Install Antrea via manifest:
         This uses [NodeIPAM within kube-controller-manager]
         ```shell
         ANTREA_VER=v2.5.1
         wget https://github.com/antrea-io/antrea/releases/download/${ANTREA_VER}/antrea.yml
         ```
      3. Modify the Antrea configuration to use the CIDRs previously chosen:
         ```text
         serviceCIDR: "10.96.0.0/16"
         serviceCIDRv6: "2001:db8:42:1::/112"
         transportInterface: "enp0s8"`
         ```
      4. Then apply it `kubectl apply -f antrea.yml`
         NOTE: The antrea.yml is over 6000 lines long. And has a Deployment
         the `antrea-controller` which take up 200m CPU and another DaemonSet
         the `antrea-agent` which will take up another 200m CPU on control-planes.
         That's 400m altogether, along with about 650m for the control-plane
         components. That's a total of 1050m CPU, so you'll need at least 2 CPUs
         per control-plane.

         NOTE: ClusterIP CIDR range for Services is only required when
         `AntreaProxy` is not enabled.
   3. As an alternative, let's install `kube-router` addon to replace `kube-proxy`
      and as our CNI of choice.
      It provides Network Policy features. `kube-router` needs to access
      kubernetes the API server to get information on pods, services, endpoints,
      network policies etc.
      1. When run as `DaemonSet`, the container image is prepackaged with `ipset`.
         As service/agent on the node, `ipset` package must be installed
         on each node.
         ```shell
         sudo apt install -y ipset ipvsadm
         ```
      2. Generate a certificate for access the `kube-apiserver`:
         ```shell
         getIps () {
             iface=${1}
             export IPV4=$(ip -4 addr show scope global dev ${iface} | grep "inet\b" | awk '{print $2}' | cut -d/ -f1)
             export IPV6=$(ip -6 addr show scope global dev ${iface} | grep "inet6\b" | awk '{print $2}' | cut -d/ -f1)
         }

         sudo mkdir -p ${cert_dir}
         cd ~/certs
         getIps "enp0s8"
         envsubst < openssl-kube-router.conf.tmpl > openssl-kube-router.conf

         cert_dir=/etc/kubernetes/pki
         CERT_DAYS=365
         CERT_BITS=2048
         CERT_CONF="openssl-kube-router.conf"
         CLUSTER_NAME=kubernetes
         CLUSTER_ENDPOINT=https://${IPV4}:6443
         i="kube-router"

         openssl genrsa -out "${i}.key" ${CERT_BITS}

         openssl req -new -key "${i}.key" -sha256 \
          -config "${CERT_CONF}" -section ${i} \
          -out "${i}.csr"

         # sign the CSR with the etcd-ca key.
         sudo openssl x509 -req -days ${CERT_DAYS} -in "${i}.csr" \
          -copy_extensions copyall \
          -sha256 -CA "${cert_dir}/ca.crt" \
          -CAkey "${cert_dir}/ca.key" \
          -CAcreateserial \
          -out "${i}.crt"
         ```
      3. Make a kubeconfig to mount inside the `kube-router`'s Static Pod, so
         it will have access to the `kube-apiserver`:
         ```shell
         mkdir -p ~/certs && cd ~/certs
         i="kube-router"
         cert_dir=/etc/kubernetes/pki
         CLUSTER_NAME=kubernetes
         CLUSTER_ENDPOINT=https://${IPV4}:6443

         # Make a kubeconfig for the node adding the cluster.
          sudo kubectl config set-cluster ${CLUSTER_NAME} \
           --certificate-authority=${cert_dir}/ca.crt \
           --embed-certs=true \
           --server=${CLUSTER_ENDPOINT} \
           --kubeconfig=/etc/kubernetes/${i}.conf
          # Add credentials to the kubeconfig.
          sudo kubectl config set-credentials ${i} \
           --client-certificate=${i}.crt \
           --client-key=${i}.key \
           --embed-certs=true \
           --kubeconfig=/etc/kubernetes/${i}.conf
         # Add a context to the cluster.
         sudo kubectl config set-context default \
           --cluster=${CLUSTER_NAME} \
           --user=${i} \
           --kubeconfig=/etc/kubernetes/${i}.conf
         # Set the context to use by default in the kubeconfig.
         sudo kubectl config use-context default \
           --kubeconfig=/etc/kubernetes/${i}.conf
         sudo mkdir -p /var/lib/kube-router
         sudo cp -p /etc/kubernetes/kube-router.conf /var/lib/kube-router/kubeconfig
         ```
      4. Make a `kube-router` manifest:
         ```shell
         mkdir ~/manifests && cd ~/manifests
         wget -O kube-router-all.yml https://raw.githubusercontent.com/cloudnativelabs/kube-router/master/daemonset/generic-kuberouter-all-features.yaml

         ```
      5. Apply changes to the `kube-router` manifest
         ```text
         wget -O kube-router-all.yaml.tmpl https://raw.githubusercontent.com/cloudnativelabs/kube-router/master/daemonset/generic-kuberouter-all-features.yaml
         ```
      6. Edit the `kube-router-all-yaml` and add a user subject to the
         `ClusterRoleBinding` like so:
         ```yaml
         ---
         kind: ClusterRoleBinding
         #...
         subjects:
         - #...
         - kind: User # Add the user to the subject for using with the kubeconfig we generated.
           name: kube-router
           apiGroup: rbac.authorization.k8s.io
         ```
      7. Fill in placeholders then apply the manifest:
         ```shell
         CLUSTERCIDR=10.96.0.0/16,2001:db8:42:1::/112 \
         APISERVER=https://control-plane:6443 \
         sh -c 'cat kube-router-all.yaml.tmpl | \
         sed -e "s;%APISERVER%;$APISERVER;g" -e "s;%CLUSTERCIDR%;$CLUSTERCIDR;g"' | tee kube-router-all.yaml
         kubectl apply -f kube-router-all.yaml
         #  --enable-cni
         #  --enable-ipv4
         #  --enable-ipv6
         #  --service-cluster-ip-range="10.96.0.0/16"
         #  --service-cluster-ip-range="2001:db8:42:1::/112"
         ```
7. Test that DNS resolution for the cluster is working:
   1. Launch a Pod with DNS tools:
      `kubectl run -it --rm --restart=Never --image=infoblox/dnstools:latest dnstools`
   2. Query the DNS server:
      `host kubernetes`
   3. Now type `exit` and it should remove the  `dnstools` Pod since we used
      `--rm` flag.

Next: [Add Workers]

---

[CoreDNS Helm chart]: https://github.com/coredns/helm
[Add Workers]: /kubernetes/4.6-add-workers.md
[NodeIPAM within kube-controller-manager]: https://antrea.io/docs/main/docs/getting-started/#nodeipam-within-kube-controller-manager