# `Setup A Virtual Environment`

Time to spin-up a local virtual environment. Then initialize each machine
by making sure all the initial software and network configurations are in
place. This prepares the machines to either serve as a control plane or worker
node.

## Spin Up the Environment

1. Install [Vagrant] and [VirtualBox] if you have not already.
2. Clone this repository locally.
3. Open a terminal to this repository and change to the `samples` directory:
   ```shell
   cd kubernetes\samples\learning
   vagrant up
   ```
   NOTE: Each machines private IP is hard-coded in the Vagrantfile so that they
   are predictable for the sake of these instructions. The
   `control-plane-01=192.168.56.11`, `worker-01=192.168.0.21`, and
   `worker-01=192.168.0.22`. If you increase the number of nodes by setting
   the environment variable `NODE_WORKERS` to a number greater than 2, then
   there IPs are sequential. There is also IPv6 support, this so you can do a
   Dualstack cluster. You'll have IPv6 `fd00:192:168:56::11` for the
   `control-plane-01`, `fd00:192:168:56::21` for `worker-01`, and
   `fd00:192:168:56::22` for `worker-02`.
   As each machine completes the boot process, you can test that you can SSH
   into the machine with `vagrant ssh <machine-hostname>`. If you need to see
   the hostname of the VMs, run `vagrant status`.
4. Optionally [Configure SSH From A JumpBox].

TIP: Open a new terminal for each machine you want to SSH into. The session
should last as long as you have the terminal open. Keep them open as long as
you may need them to save time.

## Machine Initialization

However you choose, you'll need to execute the following steps on each VM. You
can SSH into each machine using the command `vagrant ssh <machine-hostname>`.

NOTE: You MUST be in the same directory where you ran `vagrant up` to SSH into
that machine using the `vagrant ssh`; this is specific to the Vagrant `ssh`
subcommand. The _actual_ `ssh` command does not have this quirk.

Determine the init system your VM uses by running `ps -p 1 -o comm=`. You need
to know this when configuring the `kubelet` on each machine.

NOTE: Currently, Kubernetes v1.33 defaults to `systemd`. In the future it should
autodetect it.

Also [Identify the cgroup version on Linux Nodes] of a machine by running
`stat -fc %T /sys/fs/cgroup/`; for cgroup v2, the output is `cgroup2fs` or
for cgroup v1, the output is `tmpfs`.

Now lets get each machine ready:

1. Make sure the OS is up-to-date and install some tools:
   ```shell
   sudo apt update && sudo apt upgrade -y
   # apt-transport-https may be a dummy package; if so, you can skip that package
   sudo apt install -y \
      apt-transport-https ca-certificates curl gpg jq openssl wget vim
   ```
2. Swap configuration
   1. Check if swap is on `sudo swapon --show`
   2. If it is, disable it temporarily with `sudo swapoff -a`. Check
      your system docs to see how to disable it permanently.
   3. To persist this change on reboots on Ubuntu edit `sudo vi /etc/fstab` and
      add the comment marker `#` symbol at the beginning of the line containing
      the swap partition, for example:
      ```text
      #/swap.img       none    swap    sw      0       0
      ```
   4. Then save the file, with vim, you type `:`, then `x`, then hit enter.
3. Configure kernel IP forwarding:
   1. Run `sysctl net.ipv4.ip_forward` to verify "IPv4 packet forwarding" is
      set to "1".
   2. If not, then run:
      ```shell
      echo  "net.ipv4.ip_forward = 1" | sudo tee -a /etc/sysctl.d/k8s.conf
      ```
   3. Optionally [Enable IPv6 packet forwarding]:
      1. check if it is turned on with `sysctl net.ipv6.conf.all.forwarding`
      2. If not, then run:
        ```shell
        echo "net.ipv6.conf.all.forwarding = 1" | sudo tee -a /etc/sysctl.d/k8s.conf
        ```
   4. Apply sysctl params without reboot:
      ```shell
      sudo sysctl --system
      ```

   NOTE: Kubernetes requires that the kernel allows IP packet forwarding. This
   may be disabled by default.
4. This guide uses a CNI network plugin that uses `iptables` and the
   `bridge-netfilter` architecture of the Linux kernel. So we need to load the
   [br_netfilter] kernel module and enable IP packet filter for Linux based
   bridge networks.
   ```bash
   cat <<TXT | sudo tee -a /etc/modules-load.d/modules.conf
   br_netfilter
   TXT
   sudo modprobe br_netfilter
   # Enable for IPv4 and IPv6 (a.k.a dual-stack), then load (with `sysctl -p`) in sysctl settings from the file specified.
   cat <<TXT | sudo tee -a /etc/sysctl.d/k8s.conf
   net.bridge.bridge-nf-call-iptables = 1
   net.bridge.bridge-nf-call-ip6tables = 1
   TXT
   # Load in sysctl settings from the file specified
   sudo sysctl -p /etc/sysctl.d/k8s.conf
   ```

   NOTE: The [ebtables] program is a filtering tool for Linux-based bridge
   firewalls. It can be combined with the `bridge-netfilter` architecture,
   which is a part of the standard Linux kernel, to allow `iptables` to also
   filter bridged IP packets. This enables the functionality of a stateful
   transparent firewall.
5. Gather each machines private IP. You'll want to run `ip address` on each
   machine to find their private IP by observing the network interfaces.
   These IPs are actually hard-coded in the [Vagrantfile] to be. That will not
   be the case if you chose another setup. However, that will require you to
   know what you are looking for.
   On each VM we'll edit the `/etc/hosts` file so that each machine can find the
   other by name. This will help us communicate to each node by hostname instead
   of IP address, which should reduce errors when copying files around.
   ```shell
   cat <<TXT | sudo tee -a /etc/hosts
   192.168.56.11       control-plane control-plane-01
   fd00:192:168:56::11 control-plane control-plane-01
   192.168.56.21       worker-01
   fd00:192:168:56::21 worker-01
   192.168.56.22       worker-02
   fd00:192:168:56::22 worker-02
   TXT
   ```

   NOTE: The Vagrant file included in this guide uses 2 adapters `enp0s3` and
   `enp0s8`, the 1st adapter is the NAT adapter, which is used to allow our
   nodes internets access. The 2nd is the adapter for our K8s network.
   You may not see a second adapter in your own custom setup. So just use the
   IPs of the one that you have. On an AWS EC2 instance the first one `ens0` or
   whatever should be fine.
6. Update the SSH daemon on all nodes to allow keyboard-interactive
   authentication:
   ```shell
   sudo sed -i 's/KbdInteractiveAuthentication no/KbdInteractiveAuthentication yes/' /etc/ssh/sshd_config
   sudo systemctl restart ssh
   ```
   NOTE: Some VMs may have this turned this off. Without it, we will not be able
   to run `ssh-copy-id` successfully.
7. Back on the `control-plane-01` node make an SSH key, then use `ssh-copy-id`
   to allow SSH key based authentication from `control-plane-01`. This will
   allow running commands on each node remotely without having to enter a
   password (the default password is "vagrant").
   ```shell
   # you can just hit enter and leave the password blank if your just practicing.
   ssh-keygen
   ssh-copy-id worker-01
   ssh-copy-id worker-02
   ```

Next: [Install containerd]

---

[Enable IPv6 packet forwarding]: https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/dual-stack-support/#prerequisite-ipv6-forwarding
[Vagrantfile]: /kubernetes/samples/Vagrantfile
[Vagrant]: https://developer.hashicorp.com/vagrant
[VirtualBox]: https://www.virtualbox.org/
[br-netfilter]: https://ebtables.netfilter.org/documentation/bridge-nf.html
[Install containerd]: /kubernetes/3.0-install-containerd.md#install-containerd
[Identify the cgroup version on Linux Nodes]: https://kubernetes.io/docs/concepts/architecture/cgroups/#check-cgroup-version
[ebtables]: https://ebtables.netfilter.org/
[br_netfilter]: https://ebtables.netfilter.org/documentation/bridge-nf.html
