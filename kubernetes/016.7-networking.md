# Networking

## Switching, Routing, DNS

**Tools of the CLI trade**

List the networking interfaces: `ip link` or `ip address
`
NOTE: To see which IP interface is used by a node in a cluster:
1. use `k get nodes -o wide` to see the internal IP of the node
2. List the `ip link` and view which IP address is assigned to the interface.

To see an IP address assigned to those interfaces use: `ip addr`
To set IP address assigned on an interfaces use: `ip addr add <IP> dev <interface>`
NOTE: TO persist on reboot, then set in the `/etc/network/interfaces` file
List the routes in the routing table on the system: `route`
To add routes to the table use: `ip route add <IP-CIDR> via <IP-destination>`
Check if IP forwarding is available on a host: `cat /proc/sys/net/ipv4/ip_forward`:
NOTE: A value of 0 = off or 1 = on.
See all the bride interfaces on a node: `ip link show type bridge`
Show routing table entry by device: `ip route`

To add a DNS add an entry into: `/etc/resolv.conf`
ex: `cat /etc/resolv.conf` ~ "nameserver    192.168.1.100"

By default `/etc/hosts` wins over `/etc/resolv.conf`, but the order can changes in the file `/etc/nsswitch.conf`. Look for a line with the "hosts:" entry and change the order there.

You can ass an entry `search some-domain.com` to always append "some-domain.com" to any
subdomain it needs to search for. It is intelligent to know if you use a full domain, then do not append the search domain. You can also supply a list of domains to search by giving search a space separated list of domain.

A - IP to host
AAAA - IPv6 to host
CNAME - name to name mapping

Tools beside `ping`
`nslookup` - does not use /etc/hosts
`dig` - to test DNS resolution, but with more details.

Port **53** is the default port for a DNS server.

## Namespaces

Provide network isolation/containment. It has its own network interface, routing table and ARP table.

To create a network namespace (NS): `ip netns add red`
To list the NS run : `ip nets`

To run the equivalent of `ip link`, which list network interfaces, inside a namespace you can run: `ip netns exec red ip link` or `ip -n red link`, this executes the link command inside the namespace, listing its network interfaces.
You can network two namespaces together with a pipe:
```shell
# add a new namespace
ip netns add blue
# make a virtual ethernet pipe
ip link add veth-red type veth peer name veth-blue
# connect NS red to one end
ip link se veth-red netns red
# connect NS blue to the other end
ip link se veth-blue netns blue

#Now we can assign IP addresses to each veth (I think that stand for virtual ethernet)
ip -n red addr add 102.168.15.1 dev veth-red
ip -n blue addr add 102.168.15.1 dev veth-blue

# Bring the interface up:
ip -n red link set veth-red up
ip -n blue link set veth-blue up

# Test out with ping
ip netns exec red ping 192.168.15.2

# see the connection
ip netns exec red arp
```

If you have more than 2 namespaces that you want to network together, then you'll need a virtual switch, to the host, like Linus Bridge.
`ip link add v-net-0 type bridge`

NOTE: This will appear in the `ip link` command and show its status if its UP or DOWN.

To bring the virtual switch up run: `ip link set dev v-net-0 up`

We need new pipes, you can delete the existing one, since we won't need it with:
`ip -n red link del veth-red`; deleting 1 gets rid of the other.

Now you need to add pipes from each namespace to v-eth-0 bridge. Use the same process as with connecting the red to blue NS, but one end goes to the NS and the other goes to the bridge.
```shell
# connect the red NS to a pipe
ip link set veth-red netns red
# connect the switch to the other end of the pipe
ip link set veth-red-br master v-net-0
# set an IP for the pipe
ip -n red addr add 192.168.15.1 dev veth-red
# then bring it up
ip -n red link set veth-red up

# repeat for remaining namespaces
```

From the host we cannot reach the internal NS network. But since the bridge is on the host and is a network device we can give it an IP and this will allow us to reach the namespaced network:
```shell
ip addr add 192.168.15.5/24 dev v-net-0
# test it out by pinging the red v-eth.
ping 192.168.15.1
```

`iptables -t nat -A POSTRROUTING -s 192.168.15.0/24 -j MASQUERADE`

## Docker Networking

No network

host network (share the host ports)

bridge an internal private network
Docker network sees the name "bridge" but the hosts sees "docker0."
```shell
ip link add docker0 type bridge
```
Docker creates a NS for each container then connects that container to the bridge with a virtual link. Each link is numbered for example 7 and 8. The bridge end gets 7 (or odd numbers) and the container gets 8 (or even number) named endpoints. The host is has access to the bridge (remember docker0 on the host), so it can connect to containers on the bridge network. Just map an available port on the host to the container. There is a NAT to assist with forwarding traffic.


## Container Network Interface (CNI)

Standards around how a program should be developed to solve chanellengins around a container networking environment. Basically how to build the bridge network program.
Or how plugins should be developed.

`netstat -plnt`

`netstat -anp | grep etcd`

## POD Networking

## CNI in Kubernetes

All CNI network plugins are stored in `/opt/cni/bin`
Their configurations are stored in `/etc/cni/net.d` as a 01-<name>.conflist
NOTE: Format is on the site itself

Example of how to install `weave` plugin:
```shell
kubectl apply -f https://github.com/weaveworks/weave/releases/download/v2.8.1/weave-daemonset-k8s.yaml
```

## CNI Weave

To see what networking solutions a cluster is using look in the `/etc/cni/net.d`
to see what is configured.

Looking at the config, in the plugins section will tell you what binaries will be run and in what order.

## IPAM Weave

This covers
How are the virtual bridge networks assigned an IP subnet?
The CNI plugin is responsible for managing IP address assignment to PODs.

How are PODs assigned IPs?
Where is this info stored?
How are no duplicate IPs assigned?

## Post Practice Test

**Quick Tips:**
* To see what weave has configured for the IP address range you can view the logs of 1 of the weave PODs and look toward the top for `ipalloc-range`.
* To see the default gateway configured on the PODs scheduled on a node:
    1. Deploy a pod to that node.
    2. run the command: `k exec busybox -- ip route`
    3. Get the IP of the default route.

## Service Networking

Always use a service to allow PODs to directly communicate with each other. This is VERY true when dealing with deployment and replicasets that have multiple pods. The service gets an IP and acts as load balancer.

## DNS in Kubernetes

## CoreDNS

Its configuration can be located at `/etc/coredns/Corefile`, but more likely its
a ConfigMap:

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: coredns
  namespace: kube-system
data:
  Corefile: |
    .:53 {
        errors
        health {
            lameduck 5s
        }
        ready
        kubernetes cluster.local in-addr.arpa ip6.arpa {
            pods insecure
            fallthrough in-addr.arpa ip6.arpa
            ttl 30
        }
        prometheus :9153
        forward . /etc/resolv.conf
        cache 30
        loop
        reload
        loadbalance
    }
```

NOTE: The `cluster.local` is the cluster root domain. The Corefile (or Configmap of the Corefile) is where it is configured.

Each item in this config can be a plugin. The `pods insecure` is responsible for creating a DNS record for PODs in the cluster.

Any record that the DNS cannot resolve is sent to the nameserver in the file specified by `forward` plugin, which in this case is  `. /etc/resolv.conf`; this file is set to use the nameserver from the kubernetes node (this is recursive and strange).

If you need to edit the CoreDNS config, just edit the configmap and the changes should propagate automatically.

To help PODs find the DNS server:
1. There is a service name kube-dns.
2. In the PODs' `/etc/resolv.conf` file; the IP of this service is also the IP used for the `nameserver` there. This is done automatically by the `kubelet`. You should see that as the field `clusterDNS` in the `kubelet` configuration.
That should be all there is to configure DNS for the cluster.

**Quick Tips:**
* You can use `nslookup` or `host` to see the fully qualified cluster domain name of any given service in the cluster. This is because  the `/etc/resolv.conf` file also has a search entry with the cluster domains:
```text
search   default.svc.cluster.local svc.cluster.local cluster.local
```

But it only has entry for `svc` and nothing else like `pod`.

## Ingress

Ingress exposes HTTP and HTTPS routes from outside the cluster to services within the cluster. Traffic routing is controlled by rules defined on the Ingress resource.

Ingress controller handles:
Load balancing
* authentication
* SSL,
* URL-based routing configurations.

* First deploy a supported solution, att Nginx and GCP GCE Loadbalancer are currently supported and maintained by the Kubernetes project. We'll use NginX.
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-ingress-controller
spec:
  replicas: 1
  selector:
    matchLabels:
      name: nginx-ingress
  template:
    metadata:
      labels:
        name: nginx-ingress
    spec:
      containers:
        - name: nginx
          image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.2.21.0
      args:
        - /nginx-ingress-controller
        - --configmap=${POD_NAMESPACE}/nginx-configuration
      env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
      ports:
        - name: http
          containerPort: 80
        - name: https
          containerPort: 443
```
Create a configmap, even though this is blank, it make it so you can update it live.
```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: nginx-configuration
```
You then need a service to expose the ingress controller.

```yaml
apiVersion: v1
kind: Service
metadata:
  name: nginx-ingress
spec:
  type: NodePort
  ports:
    - port: 80
      targetPort: 80
      protocol: TCP
      name: http
    - port: 443
      targetPort: 443
      protocol: TCP
      name: https
  selector:
    name: nginx-ingress
```

Now you need 2 service accounts:
```yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: nginx-ingress-sa
```

You need 2 roles:
```yaml
# ingress-nginx
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  creationTimestamp: "2025-01-24T03:39:26Z"
  labels:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.1.2
    helm.sh/chart: ingress-nginx-4.0.18
  name: ingress-nginx
  namespace: ingress-nginx
  resourceVersion: "1421"
  uid: 766f31a3-af6c-42da-b7bc-82ea4932fd04
rules:
- apiGroups:
  - ""
  resources:
  - namespaces
  verbs:
  - get
- apiGroups:
  - ""
  resources:
  - configmaps
  - pods
  - secrets
  - endpoints
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - services
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - networking.k8s.io
  resources:
  - ingresses
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - networking.k8s.io
  resources:
  - ingresses/status
  verbs:
  - update
- apiGroups:
  - networking.k8s.io
  resources:
  - ingressclasses
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - ""
  resourceNames:
  - ingress-controller-leader
  resources:
  - configmaps
  verbs:
  - get
  - update
- apiGroups:
  - ""
  resources:
  - configmaps
  verbs:
  - create
- apiGroups:
  - ""
  resources:
  - events
  verbs:
  - create
  - patch
```
```yaml

apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  annotations:
    helm.sh/hook: pre-install,pre-upgrade,post-install,post-upgrade
    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
  creationTimestamp: "2025-01-24T03:39:26Z"
  labels:
    app.kubernetes.io/component: admission-webhook
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.1.2
    helm.sh/chart: ingress-nginx-4.0.18
  name: ingress-nginx-admission
  namespace: ingress-nginx
  resourceVersion: "1422"
  uid: 5d533b21-8f9c-4d45-889c-7b3180f2e9d2
rules:
- apiGroups:
  - ""
  resources:
  - secrets
  verbs:
  - get
  - create
```

You need 2 role bindings:
```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  creationTimestamp: "2025-01-24T03:39:26Z"
  labels:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.1.2
    helm.sh/chart: ingress-nginx-4.0.18
  name: ingress-nginx
  namespace: ingress-nginx
  resourceVersion: "1425"
  uid: 13aefb7f-e4cf-45be-9218-5660e90607db
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: ingress-nginx
subjects:
- kind: ServiceAccount
  name: ingress-nginx
  namespace: ingress-nginx
```
```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  annotations:
    helm.sh/hook: pre-install,pre-upgrade,post-install,post-upgrade
    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
  creationTimestamp: "2025-01-24T03:39:26Z"
  labels:
    app.kubernetes.io/component: admission-webhook
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.1.2
    helm.sh/chart: ingress-nginx-4.0.18
  name: ingress-nginx-admission
  namespace: ingress-nginx
  resourceVersion: "1426"
  uid: 88f430bc-526d-45a3-921a-999e829680e7
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: ingress-nginx-admission
subjects:
- kind: ServiceAccount
  name: ingress-nginx-admission
  namespace: ingress-nginx
```


```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: minimal-ingress
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  ingressClassName: nginx-example
  rules:
  - http:
      paths:
      - path: /testpath
        pathType: Prefix
        backend:
          service:
            name: test
            port:
              number: 80
```

Rules you configure as ingress resources.
```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ingress-resource-backend
spec:
  defaultBackend:
    resource:
      apiGroup: k8s.example.com
      kind: StorageBucket
      name: static-assets
  rules:
    - http:
        paths:
          - path: /icons
            pathType: ImplementationSpecific
            backend:
              resource:
                apiGroup: k8s.example.com
                kind: StorageBucket
                name: icon-assets
```
You need to expose it to make it available externally. Publish it as a NodePort or with a Cloud native LoadBalancer. Which is a one-time configuration.

```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
```