# kubeadm Cluster Install

1. Add the Kubernetes GPG key to be trusted on your system:
   ```shell
   curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.33/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
   ```
   NOTE: This is pointing to the version 1.33 repository.
2. Add the appropriate Kubernetes apt repository:
   ```shell
   echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.33/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list
   ```
3. Update the apt package index, install kubelet, kubeadm and kubectl, and pin their version:
   ```shell
   sudo apt-get update
   sudo apt-get install -y kubelet kubeadm kubectl
   sudo apt-mark hold kubelet kubeadm kubectl
   ```
4. You need to choose which IP CIDR ranges you want to use for IPv4 and also
   IPv6 if your going dual stack. The ones provided here are from examples in
   the Kubernetes documentation, which are private class for IPv4 and
   global unicast for IPv6:
   ```text
   IPv4 POD network:     10.244.0.0/16
   IPv4 Service Network: 10.96.0.0/16
   IPv6 POD network:     2001:db8:42:0::/56
   IPv6 Service Network: 2001:db8:42:1::/112
   ```
5. Begin installing the cluster on the control plane:
   1. Log into the control plane with `ssh vagrant@control-plane-01`.
   2. Generate a **ClusterConfiguration** object with default values by running
      `kubeadm config print init-defaults > kubeadm-config.yaml`.
   3. Modify the configuration to include the IPv4/IPv6 CIDRs, advertise
      address, and controlplane DNS endpoint of your choosing like so:
      ```yaml
      ---
      apiVersion: kubeadm.k8s.io/v1beta4
      kind: InitConfiguration
      localAPIEndpoint:
        advertiseAddress: 192.168.56.11
        bindPort: 6443
      ---
      apiVersion: kubeadm.k8s.io/v1beta4
      kind: ClusterConfiguration
      kubernetesVersion: 1.33.0
      networking:
        dnsDomain: cluster.local
        podSubnet: 10.244.0.0/16,2001:db8:42:0::/56
        serviceSubnet: 10.96.0.0/16,2001:db8:42:1::/112
      controlPlaneEndpoint: control-plane
      apiServer:
        certSANs:
         - 127.0.0.1 # When you set the advertiseAddress to a non-loopback address, you must add the loopback address to the certSANs list.
         - 192.168.56.11
      ---
      kind: KubeletConfiguration
      apiVersion: kubelet.config.k8s.io/v1beta1
      cgroupDriver: systemd
      ```
   4. start the cluster installation:
      ```shell
      sudo kubeadm config images pull
      sudo kubeadm init --config kubeadm-config.yaml
      ```

      NOTE: The following resources were used to piece all this together:
      [Before you begin], [Creating a cluster with kubeadm].

      NOTE: You can use `sudo kubeadm reset` if the init phase fails you.

      NOTE: Typically this is an excellent time to back up the CAs that kubeadm
      generated. You can get away with just the
      `/etc/kubernetes/pki/ca.{key,crt}` files because you can then use kubeadm
      to remake all the others. But it is just as easy to just back up the
      entire `/etc/kubernetes/pki` directory.
6. Before we can add any worker nodes machines we need to add a CNI network.

   NOTE: For ease of example, were going to use Flannel. However, Flannel out of
   the box requires the br_netfilter kernel module to ensure network traffic
   crossing the CNI `vxlan` network is processed by `iptables`. See
7. [Machine Initialization] and search for `br_netfilter` on how to load it.

   1. Download the Flannel manifest, rather than applying it, so we can modify
      it:
      ```shell
      wget https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml
      ```
   2. Edit the `vi kube-flannel.yml` to point Flannel to the correct network
      interface, such as `--iface=enp0s8`:
      ```yaml
      containers:
       - args:
         - --ip-masq
         - --kube-subnet-mgr
         - --iface=enp0s8
      ```
      NOTE: Be careful, even if you use this [Vagrantfile], the network card may
      have a different name, be sure to check what name it has with `ip address`
      on the control plane VM.
   3. Optionally edit `vi kube-flannel.yml` to add IPv6:
      ```yaml
      data:
        net-conf.json: |
          {
            "Network": "10.244.0.0/16",
            "EnableNFTables": false,
            "Backend": {
             "Type": "vxlan"
            },
            "EnableIPv6": true,
            "IPv6Network": "2001:db8:42:0::/56"
          }
      ```
   4. Apply the Flannel CNI network addon:`kubectl apply -f kube-flannel.yml`
      NOTE: Run `kubectl get pods -A` and verify that the kube-flannel and
      coredns pods are all running.

      NOTE: If all containers are not running, review their logs to try and find
      the issue.
8. Run "kubeadm join..." command on each worker node.

   NOTE: Run `kubectl get nodes` to verify they are all in the ready state.

---

[Vagrantfile]: /kubernetes/samples/Vagrantfile
[Machine Initialization]: /kubernetes/2.0-setup-a-virtual-environment.md#machine-initialization
