# Configure Networking

Before we begin, lets see what bridge networks we have so far:
`sudo bridge link show`. Just make a mental note, or paste the output for
reference later.

1. Add a default kubeconfig to access the cluster.
   ```shell
   mkdir -p ~/.kube
   sudo cp /etc/kubernetes/admin.conf ~/.kube/config
   sudo chown vagrant:vagrant ~/.kube/config
   chmod 0700 ~/.kube/config
   sudo kubectl create clusterrolebinding \
     kubernetes-admin --clusterrole=cluster-admin --user=kubernetes-admin \
     --kubeconfig /etc/kubernetes/super-admin.conf
   ```
2. Copy the `kube-proxy.tmpl.yaml` file to `control-plane-01`.
3. Fill in the values for the `kube-proxy` config and then apply the manifest.
   ```shell
   export ADVERTISE_ADDRESS=192.168.56.11
   export POD_CIDRS=10.244.0.0/16,2001:db8:42:0::/56
   export SERVICE_CIDRS=10.96.0.0/16,2001:db8:42:1::/112
   export KUBE_VER=v1.33.3
   envsubst < kube-proxy.tmpl.yaml | tee kube-proxy.yaml
   kubectl apply -f kube-proxy.yaml
   ```
4. Install Helm
   ```shell
   wget https://get.helm.sh/helm-v3.18.4-linux-amd64.tar.gz
   sudo tar Czxvf /usr/local/bin/ helm-v3.18.4-linux-amd64.tar.gz --strip-components=1
   ```
5. Install CoreDNS using the [CoreDNS Helm chart].
   ```shell
   cp /vagrant/coredns-values.yaml .
   helm repo add coredns https://coredns.github.io/helm
   helm repo update
   helm --namespace=kube-system install coredns coredns/coredns -f coredns-values.yaml
   ```
6. Install Flannel CNI:
   1. Download the manifest.
      ```shell
      wget https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml
      ```
   2. Edit container spec to set the network interface to use:
      ```yaml
      containers:
       - args:
         - --ip-masq
         #...
         - --iface=enp0s8
      ```
      NOTE: Be careful, even if you use this [Vagrantfile], the network card may
      have a different name, be sure to check what name it has with `ip address`
      on the control plane VM.
   3. Optionally edit `vi kube-flannel.yml` to add IPv6:
      ```yaml
      data:
        net-conf.json: |
          {
            "Network": "10.244.0.0/16",
            "EnableNFTables": false,
            "Backend": {
             "Type": "vxlan"
            },
            "EnableIPv6": true,
            "IPv6Network": "2001:db8:42:0::/56"
          }
      ```
7. As an alternative to Flannel if you want to get practice with network
   policies, install the Antrea addon.
   1. Install OvS on all nodes.
      ```shell
      sudo apt install -y openvswitch-switch openvswitch-common
      ```

      NOTE: See https://docs.openvswitch.org/en/latest/intro/install/debian/#installing-deb-packages

   2. Install Antrea via manifest:
      This uses [NodeIPAM within kube-controller-manager]
      ```shell
      ANTREA_VER=v2.4.2
      wget https://github.com/antrea-io/antrea/releases/download/${ANTREA_VER}/antrea.yml
      kubectl apply -f antrea.yml
      ```
      set
      ```text
      serviceCIDR: "10.96.0.0/16"
      serviceCIDRv6: "2001:db8:42:1::/112"
      transportInterface: "enp0s8"`

      IPv4 POD network:     10.244.0.0/16
      IPv6 POD network:     2001:db8:42:0::/56
      ```
      NOTE: The antrea.yml is over 6000 lines long. And has a Deployment
      the `antrea-controller` which take up 200m CPU and another DaemonSet
      the `antrea-agent` which will take up another 200m CPU on control-planes.
      That's 400m altogether, along with about 650m for the control-plane
      components. That a total of 1050m CPU, so you'll need at least 2 CPUs per
      control-plane.

      ClusterIP CIDR range for Services is only required when AntreaProxy is not
      enabled.
8. Test that DNS resolution for the cluster is working:
   1. Launch a Pod with DNS tools:
      `kubectl run -it --rm --restart=Never --image=infoblox/dnstools:latest dnstools`
   2. Query the DNS server:
      `host kubernetes`
   3. Now remove the test pod with `kubectl delete pod dnstools`.

Next: [Add Workers]

---

[CoreDNS Helm chart]: https://github.com/coredns/helm
[Add Workers]: /kubernetes/4.6-add-workers.md
[NodeIPAM within kube-controller-manager]: https://antrea.io/docs/main/docs/getting-started/#nodeipam-within-kube-controller-manager