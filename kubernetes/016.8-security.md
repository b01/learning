# Security

## Security Primitives

Risk:
* Controlling access to the API Server
  * Users and passwords
  * user and tokens
  * certificates
  * Eternal Auth providers e.g. LDAP
  * Service accounts

Measures to take with Risk Mitigate :
* RBAC Authorization
* ABAC Authorization
* Node Authorization
* Webhook Mode

Kubernetes does NOT manage user accounts natively, it relies on an external source like LDAP.

## Authentication

* Static passwords and tokens

Create a list of users and their passwords in a CSV file and use that as source.
Such as __password,username,userid__, for example:

user-details.csv:
```csv
password123,user1,u0001
```

You can pass whit file as an options to the  `kube-apiserver --basic-auth-file=user-details.csv`. You MUST restart the kube-apiserver for these to take effect. If you setup with `kubeadm` then you just modify the `/etc/kubernetes/manifests/kube-apiserver.yaml manifest and it will be automatically restarted.

`curl -v -k https://master-no-ip:6443/api/v1/pods -u "user1:passs1"`

In the CSV file for users you can have an optional column `group` you can also have a CSV that list user tokens.

user-token-details.csv:
```csv
adfsfdsfdsfas,user10,u0010,dev
adfsfdsfdsfas,user11,u0011,admin
```

`--token-auth-file=user-token-details.csv`

```shell
curl -v -k https://master-node-ip:6443/api/v1/pods --header "Authorization: Bearer <token>"`

```

Note: This is not recommended in a production environment. This is only for learning purposes. Also note that this approach is deprecated in Kubernetes version 1.19 and is no longer available in later releases.

```yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  name: kube-apiserver
  namespace: kube-system
spec:
  containers:
  - name: muname
    command:
    - kube-apiserver
    - --authorization-mode=Node,RBAC
    - --basic-auth-file=/tmp/users/user-details.csv
```

Create the necessary roles and role bindings for these users:

```yaml
---
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  namespace: default
  name: pod-reader
rules:
- apiGroups: [""] # "" indicates the core API group
  resources: ["pods"]
  verbs: ["get", "watch", "list"]

---
# This role binding allows "jane" to read pods in the "default" namespace.
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: read-pods
  namespace: default
subjects:
- kind: User
  name: user1 # Name is case sensitive
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role #this must be Role or ClusterRole
  name: pod-reader # this must match the name of the Role or ClusterRole you wish to bind to
  apiGroup: rbac.authorization.k8s.io
```
Once created, you may authenticate into the kube-api server using the users credentials

```shell
curl -v -k https://localhost:6443/api/v1/pods -u "user1:password123"
```

## TLS

Is used to guarantee trust between a user and the server.

Symmetric Encryption - uses the same key to encrypt and decrypt data. Since the key has to also be sent to the server, there is a risk of the key getting out and then anyone with it can decrypt the data.
```shell
ssh-keygen
```
Asymmetric Encryption - uses a pair of keys, private key and a public key. The public key is shared openly and anyone can use it to lock things. Only users with access to the private key can access the data.

```shell
openssl genrsa -out my-bank.key 1024
openssl rsa -in my-bank.key -pubout > mybank.pem
```

## Certificates API

Kubernetes API for managing certificates signing.

1. user submits a csr
2. the csr mub be added as a resource to kubernetes and also must be base64 encoded and added to the resource. You can do this in a yaml file:
```yaml
apiVersion: certificates.k8s.io/v1
kind: CertificateSigningRequest
metadata:
  name: akshay
spec:
  groups:
    - system:authenticated
  request: <Paste the base64 encoded value of the CSR file>
  signerName: kubernetes.io/kube-apiserver-client
  usages:
    - client auth
```

The controller Manager is responsible for processing CSRs. It has component for CSR signing and approving. It uses the kube-apiserver's certificates to do so:
```yaml
spec:
  containers:
    - command:
      - kube-controller-manager
      - --address=127.0.0.1
      - -- cluster-signing-cert-file=/etc/kubernetes/pki/ca.cert
      - -- cluster-signing-key-file=/etc/kubernetes/pki/ca.key
```

## KubeConfig

Quick tip: Sending a curl request with a cert:

```shell
curl https://my-kube-playground:6443/api/v1/pods \
  --key admin.key \
  --cert admin.crt \
  --cacert ca.crt
```
or -

```shell
k get pods \
  --server my-kube-playground:6443 \
  --client-key admin.key \
  --client-certificate admin.crt \
  --certificate-auth ca.cert
```

The configuration file has 3 sections:

* clusters: that you need access to
* contexts: user that you can authenticate as
* users: define with account will be used to access a cluster.
---

With the config you are using existing users to access clusters. It makes it convenient to access the cluster by specifying auth credentials in the config so that you do not need to type them out everytime, like in the example above.


```yaml
apiVersion: v1
kind: Config

# you can set a default context ;-)
current-context: mycluster

clusters:
  - name: mycluster
    cluster:
      certifacte-authority: ca.crt
      server: https://localhost:6443

contexts:
  - name: mycluster
    context:
      cluster: mycluster
      user: rando
      namespace: finance

users:
  - name: rando
    user:
      client-certificate: admin.crt
      client-key: admin.key
```

you can use the `certificate-authority-data` field and supply the base64 encoded contents of the certificate directly in the KubeConfig.

## API Groups

What are they:
```shell
curl https://localhost:6443/version
curl https://localhost:6443/api/v1/pods
```

Groups in this case refer to the API group you want to use, the in curl commands that would be "version" and "api." There are also:
* metrics - used to monitor the health of the cluster
* healthz - used to monitor the health of the cluster
* apis - named; are more organized and going forward will be used. apps, exetensions, networking.k8s.io, etc.
* api  - core; such namespaces, pods, rc, etc.
* logs - for integrating with 3rd party logging applications

To access the APIs you musht authenticate ourside of `/version`. Or you can use the `k proxy` command which will use the certificates in the KubeConfig. This is NOT the same as kube-proxy service running int the controlplane.

## Authorization

Defines what can a user do once they are authenticated.

Types of Auth:

* Node
* ABAC
* RBAC
* AlwaysAllow - used by default if no others are specified.
* AlwaysDeny

When you enable multiple modes of authorization, they are tried 1 at a time until a request is approved or they all deny the request.

## RBAC

To make a role:

```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: developer
rules:
  - apiGroups: [""] # again leave it blank for core group.
    resources: [ "pods" ]
    verbs: [ "list", "get", "create", "update", "delete"]
  - apiGroups: [""] # again leave it blank for core group.
    resources: [ "ConfigMap" ]
    verbs: [ "list", "get", "create", "update", "delete"]
```

To link a user to a role, you need a binding object:

```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: developer-devuser
  # to limit a user to a namespace, add that here
subjects:
  - kind: User
    name: dev-user
    apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: developer
  apiGroup: rbac.authorization.k8s.io
```

QuickTips:

To validate if you have access, then you can use the `can-i` command:

```shell
k auth can-i create deployment
k auth can-i delete nodes
```

Test a user
```shell
k auth can-i create deployment --as <userName>
k auth can-i delete nodes --as <userName>
```

Just use --namespace to check if the user has access to a namespace. Add the "resourceNames" field to the rules section to further restrict user to certain resources.

Create the necessary roles and role bindings required for the dev-user to create, list and delete pods in the default namespace.

```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: developer
rules:
  - apiGroups: [""] # again leave it blank for core group.
    resources: [ "pods" ]
    verbs: [ "create", "delete", "list" ]
```

```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: dev-user-binding
  # to limit a user to a namespace, add that here
  namespace: default
subjects:
  - kind: User
    name: dev-user
    apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: developer
  apiGroup: rbac.authorization.k8s.io
```

## Cluster Roles

Are roles for Resources that are grouped at the cluster level, such as
nodes, clusterroles, PV, certificatesigningrequest, namespaces., etc.


```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: cluster-admin
rules:
  - apiGroups: [""] # again leave it blank for core group.
    resources: [ "nodes" ]
    verbs: [ "create", "delete", "get", "list" ]
```

```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: cluster-admin-binding
  # to limit a user to a namespace, add that here
  namespace: default
subjects:
  - kind: User
    name: cluster-admin
    apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: cluster-admin
  apiGroup: rbac.authorization.k8s.io
```

ClusterRoles can also be used for user to access namespaced resources as well.

## Service Accounts

Are used my automated processes to do interact with the Kubernetes cluster.

When you make a service account after version 1.24 you will need to also make a token as it is no longer automatically generated.

```shell
k create serviceaccount dashboard-sa
k get serviceaccount
k descrie serviceaccount dashboard-sa
k create token dashboard-sa
# Token can be used as an auth header in curl.
k descrie seceret dashboard-sa-token-asbd
```

If you host an app in the cluster, you can mount the secret into the pod.

For every namespace in the cluster a "default" service account. Whenever a POd is made, that default service account and its token are mounted, as a volume mount, and to the pod automatically. It has restricted permissions to only run basic queries.

NOTE: You cannot edit the service account of an existing POD, you MUST replace the POD. For deployments automatically replace the PODs if you edit the service account.

You can use `spec.automountServiceAccountToken: false` to skip mounting the default service account to the pod.

`/var/run/secrets/kubernetes.io/serviceaccount`

## Image

To use a private repository:
````shell
kubectl create secret docker-registry regcred \
  --docker-server=<your-registry-server> \
  --docker-username=<your-name> --docker-password=<your-pword> \
  --docker-email=<your-email>
````

Inspect the secret:

```shell
kubectl get secret regcred --output=yaml
```

Create a Pod that uses your Secret:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: private-reg
spec:
  containers:
  - name: private-reg-container
    image: <your-private-image>
  imagePullSecrets:
  - name: private-reg-cred


```

Docker security

```yaml

spec:
  securityContext:
    *options
```

Network Policy

1. Add labels to your POD.
2. Create a network policy with selectors to specify a POD.

Blocking only occurs for the direction of traffic listed in the policy. So

if you specify the ingress, then only ingress traffic will be enforced. While egress would be allowed as normal. They are stateful.

Flannel does NOT support network policies, but you can still create them.
